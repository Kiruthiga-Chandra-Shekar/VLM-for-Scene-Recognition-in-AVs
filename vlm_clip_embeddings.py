# -*- coding: utf-8 -*-
"""VLM CLIP Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QvpiqeoYFQPfGm81j532h9-eOHTi_Uti

Contrastive Language-Image Pretraining (CLIP)
"""

from transformers import CLIPTokenizer, CLIPProcessor, CLIPModel
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
import requests
from io import BytesIO

"""Load Model"""

# Define the model name for the CLIP variant (Vision Transformer - base, 32x32 patches)
model_name = "openai/clip-vit-base-patch32"

# Load the pre-trained CLIP model from HuggingFace
model = CLIPModel.from_pretrained(model_name)

"""Tokenize Text Strings"""

# Load the tokenizer associated with the specified CLIP model
tokenizer = CLIPTokenizer.from_pretrained(model_name)

# Define a list of text descriptions to embed
text = ["a car", "a pedestrian", "a bicyclist", "a baby"]

#Tokenize and preprocess the text imputs with padding to ensure equal sequence lengths
inputs = tokenizer(text, padding=True, return_tensors="pt")

#Unpack the inputs
input_ids = inputs.input_ids #Tokenized and encoded text input IDs

#Print information about the unpacked inputs
print("Input IDs (shape):", input_ids.shape)
print(input_ids)

"""Find Text Embeddings"""

#Compute text embeddings without tracking gradients (inference mode)
with torch.no_grad():
  # Obtain text embeddings (feature vectors) from the CLIP model
  text_embeddings = model.get_text_features(**inputs)
# Print the shape of the resulting text embeddings tensor
#The shape is number of texts, embedding dimension
print(text_embeddings.shape)

"""Calculate Cosine Similarity"""

# Assuming text_embeddings is a tensor of shape [n,d], where:
# - n = number of text prompts
# - d = embedding dimension
#Compute the nxn cosine similarity matrix between all pairs of embeddings
#text_embeddings[:, None, "] reshapes embeddings to [n, 1, d]
#text_embeddings[None, :, :] reshapes embeddings to [1, n, d]
#cosine_similarity calculates similarity along the last dimension (d)

cosine_similarity = F.cosine_similarity(
    text_embeddings[:, None, :],
    text_embeddings[None, :, :],
    dim=2                       #Calculate similarity along embedding dimension d
).cpu().numpy()                 # Move to CPU and convert tensor to NumPy array for plotting

#Initialize a matplotlib figure with a specified size
plt.figure(figsize=(6,4))

#Create a heatmap visualization using seaborn to display the cosine similarity matrix
sns.heatmap(
    cosine_similarity,
    annot=True,
    fmt=".2f",
    cmap="coolwarm",
    xticklabels=text,
    yticklabels=text
)

plt.title("Cosine Similarity Between Text Embeddings")
plt.xlabel("Text Embeddings")
plt.ylabel("Text Embeddings")
plt.show()

"""Plot Images"""

# Utility function for displaying images with labels
def plot_images(images, labels):
  n = len(images)
  if n == 0:
    print("No images to display.")
    return
  fig, axes = plt.subplots(1, n)
  #Loop through each subplot axis, image and its label to display them
  if n == 1:
    ax = axes # Get the single Axes object
    ax.imshow(images[0])
    ax.set_title(labels[0])
    ax.axis("off")
  else:
    for ax, img, lbl in zip(axes, images, labels):
      ax.imshow(img)
      ax.set_title(lbl)
      ax.axis("off")
  plt.tight_layout()
  plt.show()

#Load a pretrained CLIP processor for handling images and text preprocessing
processor = CLIPProcessor.from_pretrained(model_name)

# Dictionary containing labels and their corresponding image urls
image_urls = {
    "a car": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTdgSmA23WWQ8cLmEV8QnYqfaiQR1d8vH0whNAMxIuVaOQ0xd0IpCfserY&s",
    "a pedestrian": "https://media.istockphoto.com/id/478525372/photo/woman-crossing-the-street-at-pedestrian-crossing.jpg?s=612x612&w=0&k=20&c=eLWbO3l_mLfgUYDadVagNAKpeuJ_DuRFby3V8H3gqLQ=",
    "a bicyclist": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR41I-Eq6O3cuxhbEsjvyn7brjSIVFSYWu3fwy6J3OHDSF6ofJoYCmR0UE&s",
    "a baby": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQwdJiUVfJug1ZoPCR3eAEvUoMiySPQZoYrvw&s"
}

#Extract the list of labels from the dictionay keys
labels = list(image_urls.keys())

#Define a robust function to load images from urls
def load_image(url):
  headers = {'User-Agent': 'Mozilla/5.0'}
  response = requests.get(url, headers=headers)
  response.raise_for_status()
  # Open the downloaded image, convert it to RGB format, and return the PIL image
  return Image.open(BytesIO(response.content)).convert("RGB")

# Initialize empty lists for successfully loaded images and their labels
images = []

#Loop through each label to load the associated image
for label in labels:
  try:
    img = load_image(image_urls[label])
    images.append(img)
  except requests.exceptions.RequestException as e:
    print(f"Failed to load {label}: {e}")
plot_images(images, labels)

"""Calculate Image Embeddings and Display Similarity"""

# Preprocess images using CLIP processor to prepare for embedding generation
image_inputs = processor(images=images, return_tensors="pt")

with torch.no_grad():
  image_embeddings = model.get_image_features(**image_inputs)

print(image_embeddings.shape)
img_similarity = F.cosine_similarity(image_embeddings[:, None, :], image_embeddings[None, :, :], dim=2).cpu().numpy()

# Plot heatmap
plt.figure(figsize=(8,6))
sns.heatmap(img_similarity, annot=True, xticklabels=labels, yticklabels=labels, cmap="coolwarm")
plt.xlabel("Image Embeddings")
plt.ylabel("Image Embeddings")
plt.title("Cosine Similarity Between Image Embeddings")
plt.show()

"""Calculate Image-Text Similarity"""

#Compute Similarity Matrix
txt_image_similarity = F.cosine_similarity(text_embeddings[:, None, :], image_embeddings[None, :, :], dim=2).cpu().numpy()

#Plot heatmap
plt.figure(figsize=(8,6))
sns.heatmap(txt_image_similarity, annot=True, xticklabels=labels, yticklabels=labels, cmap="coolwarm")
plt.xlabel("Text Embeddings")
plt.ylabel("Image Embeddings")
plt.title("CLIP Image-Text Similarity Heatmap")
plt.show()